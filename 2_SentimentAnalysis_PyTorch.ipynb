{"cells":[{"cell_type":"markdown","metadata":{"id":"CRxPDBg36Rau"},"source":["# Sentiment Analysis 2 - PyTorch\n","\n","## 0. Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":26720,"status":"ok","timestamp":1633959195508,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"XBaJQYGQ6Raz"},"outputs":[],"source":["# General packages\n","import os\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import copy\n","import random\n","import sys\n","try: \n","    import bcolz\n","except ImportError:\n","    # pip install bcolz causes conflicts\n","    !conda install -c anaconda --yes --prefix {sys.prefix} bcolz \n","    import bcolz\n","import pickle\n","\n","# NLP packages\n","import nltk\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Modelling packages\n","from torch.utils.tensorboard import SummaryWriter\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, classification_report\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch.nn.utils import clip_grad_norm_\n","\n","from src.text_classifiers import RNNClassifier, initialize_weights\n","from src.tripadvisor_dataset import SentimentDataset, collate\n","from src.utils import calc_distr, parse_glove\n","\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (4.0, 3.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","plt.rc('xtick', labelsize=12)    # fontsize of the tick labels\n","plt.rc('ytick', labelsize=12)    # fontsize of the tick labels\n","plt.rc('legend', fontsize=10)    # legend fontsize\n","\n","# for auto-reloading external modules see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"YUEPOfTR6Ra2"},"source":["## Parameters"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":295,"status":"ok","timestamp":1633959195793,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"V2sm6pxH6Ra3"},"outputs":[],"source":["params={\n","    'SEED': 123,            # seed for random processes                 \n","    'max_features': 20000,  # vocabulary size\n","    'embedding_dim': 256,   # size of an embedding vector (if not using pretrained embedding vectors)\n","    'hidden_size': 512,     # number of features in the hidden state of the rnn layer\n","    'rnn_type': 'lstm',      # use 'rnn', 'lstm' or 'gru'\n","    'bidirectional': True,  # If True, use bidirectional rnn \n","    'n_layers': 5,          # number of layers in the stacked rnn \n","    'dropout': 0.5,         # if non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer\n","    'output_size': 3,       # number of classes\n","    'gclip': 40,            # To tackle the exploding gradient problem, set gclip and use clip_grad_norm_(X, gclip)\n","    'batch_size': 128,       # Batch size for stochastic gradient descent\n","    'lr': 5e-4,             # Learning rate for the Adam optimizer\n","    'lr_decay': 0.5,       # Learning rate decay parameter for ReduceLROnPlateau\n","    'epochs': 100,           # Number of epochs to train the model\n","    'print_every': None,    # Frequency of printing the loss in an epoch\n","    'patience': 10,          # Patience value used for early stopping if the performance does not increase.\n","    'weighted_loss': True,  # Use a weighted Cross Entropy Loss function to balance classes\n","    'pretrained_embedding': 300    # Use pretrained embedding vectors, otherwise train from scratch. Can be 50, 100, 200, 300 or None\n","}"]},{"cell_type":"markdown","metadata":{"id":"pqcjOODs6Ra4"},"source":["## 1. Dataset\n","### Load Data"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"executionInfo":{"elapsed":1442,"status":"ok","timestamp":1633959197230,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"e4eyZwDJ6Ra5","outputId":"7007ae7e-aecd-44fa-e273-f4860d7f734e"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Review</th>\n","      <th>Rating</th>\n","      <th>Review_preprocessed</th>\n","      <th>Sentiment_rating</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>nice hotel expensive parking got good deal sta...</td>\n","      <td>4</td>\n","      <td>nice hotel expensive parking got good deal sta...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ok nothing special charge diamond member hilto...</td>\n","      <td>2</td>\n","      <td>ok nothing special charge diamond member hilto...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>nice rooms not 4* experience hotel monaco seat...</td>\n","      <td>3</td>\n","      <td>nice room experience hotel monaco seattle good...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>unique, great stay, wonderful time hotel monac...</td>\n","      <td>5</td>\n","      <td>unique great stay wonderful time hotel monaco ...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>great stay great stay, went seahawk game aweso...</td>\n","      <td>5</td>\n","      <td>great stay great stay went seahawk game awesom...</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              Review  ...  Sentiment_rating\n","0  nice hotel expensive parking got good deal sta...  ...                 1\n","1  ok nothing special charge diamond member hilto...  ...                 0\n","2  nice rooms not 4* experience hotel monaco seat...  ...                 1\n","3  unique, great stay, wonderful time hotel monac...  ...                 2\n","4  great stay great stay, went seahawk game aweso...  ...                 2\n","\n","[5 rows x 4 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["data_root = os.path.join(os.path.abspath(os.getcwd()), \"data\", \"tripadvisor_hotel_reviews_preproc.csv\")\n","data = pd.read_csv(data_root)\n","data.head()"]},{"cell_type":"markdown","metadata":{"id":"KBYi01ja6Ra7"},"source":["### Create Vocabulary\n","\n","Build a vocabulary that only consider the top `max_features` ordered by term frequency across the corpus."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1905,"status":"ok","timestamp":1633959199132,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"BO6oxUfD6Ra7"},"outputs":[],"source":["\n","tfidfconverter = TfidfVectorizer(use_idf=True, max_features=params['max_features'], min_df=0, max_df=1.0)\n","tfidfconverter.fit_transform(data['Review_preprocessed'])\n","pre_vocab = tfidfconverter.vocabulary_"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1633959199134,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"Zu_sVg4Z6Ra8","outputId":"e55fa0d5-81c2-43e4-accb-d9936ad7b299"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary size: 20000 \n","\n","  Sample words\n","--------------------\n"," swamped\n"," owns\n"," loofah\n"," unmarked\n"," straight\n"," favor\n"," peeled\n"," prudential\n"," excersice\n"," bbq\n"]}],"source":["print('Vocabulary size:', len(pre_vocab), '\\n\\n  Sample words\\n{}'.format('-' * 20))\n","sample_words = random.sample(list(pre_vocab.keys()), 10)\n","for word in sample_words:\n","    print(' {}'.format(word))"]},{"cell_type":"markdown","metadata":{"id":"_CfkB-t46Ra9"},"source":["In addition to the words that appear in our data, we need to have two special words:\n","\n","- `<eos>` End of sequence symbol used for padding\n","- `<unk>` Words unknown in our vocabulary"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1633959199134,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"YXYeyY_Q6Ra9","outputId":"c981c14b-edb9-4954-eea4-2b580676daed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary size: 20002\n"]}],"source":["vocab = {'<eos>': 0, '<unk>': 1}\n","for key in pre_vocab.keys():\n","    vocab[key] = len(vocab)\n","print('Vocabulary size:', len(vocab))\n","# for i in range(len(vocab)-50, len(vocab)):\n","#     print(list(vocab.keys())[list(vocab.values()).index(i)])"]},{"cell_type":"markdown","metadata":{"id":"Fjc5rE886Ra-"},"source":["### Create Dataset\n","\n","Steps:\n","* Tokenize data\n","* Create index-label pairs based on vocabulary\n","* Divide into train, validation and test sets\n","* Wrapping to PyTorch dataset\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10970,"status":"ok","timestamp":1633959210102,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"1-3WCSBE6Ra_","outputId":"01ddcbbf-4ddf-4bd1-f8de-22c6c294c6ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["(['people', 'talking', 'ca', 'believe', 'excellent', 'rating', 'hotel', 'yes', 'patricia', 'extremely', 'helpful', 'fluent', 'language', 'go', 'way', 'make', 'welcome', 'said', 'place', 'bit', 'dump', 'inexpensive', 'hotel', 'expensive', 'city', 'place', 'bit', 'dated', 'institutional', 'odor', 'charm', 'funeral', 'home', 'walking', 'step', 'hotel', 'girlfriend', 'step', 'condom', 'yes', 'condom', 'step', 'lot', 'guy', 'hanging', 'desk', 'hallway', 'girlfriend', 'swears', 'house', 'prostitution', 'patricia', 'arrange', 'taxi', 'following', 'morning', 'stayed', 'night', 'wrong', 'information', 'cost', 'fare', 'room', 'clean', 'large', 'bathroom', 'small', 'passable', 'night', 'glad', 'leave', 'following', 'morning', 'recommend', 'extended', 'stay', 'unless', 'tight', 'budget', 'care', 'look', 'feel', 'place'], 0)\n"]}],"source":["tokenized_data = []\n","for i in range(len(data)):\n","    tokenized_data.append((nltk.word_tokenize(data['Review_preprocessed'][i]), data['Sentiment_rating'][i]))\n","print(tokenized_data[-1])    "]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":757,"status":"ok","timestamp":1633959210850,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"Ue5j2wE16RbA","outputId":"388b6cba-eaef-4700-9072-2e7e5ace7800"},"outputs":[{"name":"stdout","output_type":"stream","text":["([49, 50, 639, 1415, 324, 1323, 3, 971, 12658, 791, 148, 11451, 390, 1660, 284, 177, 479, 98, 449, 972, 2149, 2883, 3, 4, 393, 449, 972, 2828, 12714, 2136, 601, 13212, 571, 66, 1202, 3, 385, 1202, 6781, 971, 6781, 1202, 552, 2119, 1364, 96, 51, 385, 1, 436, 11113, 12658, 252, 373, 1037, 42, 456, 41, 905, 1754, 804, 1099, 27, 28, 219, 92, 549, 6704, 41, 2676, 415, 1037, 42, 450, 2219, 9, 853, 2062, 1360, 839, 298, 361, 449], 0)\n"]}],"source":["indexed_data = []\n","for tokens, label in tokenized_data:\n","    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]    \n","    # the token that is not in vocab get assigned <unk>\n","    indexed_data.append((indices, label))\n","print(indexed_data[-1])    "]},{"cell_type":"markdown","metadata":{"id":"aeGDkXLf6RbA"},"source":["Dataset class also reverse sorts the sequences with respect to the lengths. Thanks to this sorting, we can reduce the total number of padded elements, which means that we have less computations for padded values."]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1633959210851,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"81xDpLYi6RbA"},"outputs":[],"source":["combined_data = [(raw_text, tokens, indices, label) for (raw_text, label), (tokens, _), (indices, _)\n","    in zip(list(data[['Review_preprocessed', 'Sentiment_rating']].to_records(index=False)), tokenized_data, indexed_data)]"]},{"cell_type":"markdown","metadata":{"id":"rWqL85z-6RbB"},"source":["Let's divide the dataset into train, validation and test sets (60%-20%-20%). Stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":296,"status":"ok","timestamp":1633959211143,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"qUeXpOD_6RbC"},"outputs":[],"source":["val_test_len = int(len(combined_data) * 0.2)\n","train_val_data, test_data = train_test_split(combined_data,test_size=val_test_len, random_state=params['SEED'], stratify=data['Sentiment_rating'])\n","df = pd.DataFrame(train_val_data, columns=['raw_text', 'tokens', 'indices', 'label'])\n","train_data, val_data = train_test_split(train_val_data,test_size=val_test_len, random_state=params['SEED'], stratify=df['label'])"]},{"cell_type":"markdown","metadata":{"id":"N4F-uqaH6RbC"},"source":["Let's check out the distribution of the classes in the created datasets."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1633959211144,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"yp4ijZcy6RbD","outputId":"1620ad57-6a52-4b8d-d8f6-7127b35538df"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'0': 0.15684934849446097, '1': 0.4012981308867308, '2': 0.44185252061880825}\n","{'0': 0.15683523455133289, '1': 0.4013298359055695, '2': 0.44183492954309767}\n","{'0': 0.15681171207808053, '1': 0.40130134200894674, '2': 0.44188694591297273}\n","{'0': 0.15690580771107857, '1': 0.40141532454856027, '2': 0.44167886774036114}\n","{'0': 0.15690580771107857, '1': 0.4011713030746706, '2': 0.4419228892142509}\n"]}],"source":["print(calc_distr(combined_data))\n","print(calc_distr(train_val_data))\n","print(calc_distr(train_data))\n","print(calc_distr(val_data))\n","print(calc_distr(test_data))"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1633959211144,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"qZopNmnT6RbD","outputId":"86bdaf4e-dd7b-40b4-f123-40f752002bcc"},"outputs":[{"name":"stdout","output_type":"stream","text":["20491\n","16393\n","12295\n","4098\n","4098\n"]}],"source":["# Check out the number of data samples in the created datasets.\n","print(len(combined_data))\n","print(len(train_val_data))\n","print(len(train_data))\n","print(len(val_data))\n","print(len(test_data))"]},{"cell_type":"markdown","metadata":{"id":"R0OcUCWW6RbD"},"source":["Let us wrap our data in a PyTorch dataset. For more details, check out the previous notebook and the corresponding dataset class defined in `src/tripadvisor_dataset.py`."]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1633959211145,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"7X8d4NMW6RbE"},"outputs":[],"source":["#Define a Dataset Class for train, val and test set\n","train_dataset = SentimentDataset(train_data)\n","val_dataset = SentimentDataset(val_data)\n","test_dataset = SentimentDataset(test_data)"]},{"cell_type":"markdown","metadata":{"id":"3kIxFwGA6RbE"},"source":["Note that in the dataset we created, not all sequences have the same length. Therefore, we cannot minibatch the data trivially. This means we cannot use a `DataLoader` class easily.\n","\n","To solve the problem, we need to pad the sequences with <code> <eos> </code> tokens that we indexed as zero. To integrate this approach into the Pytorch <a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\" target=\"_blank\">Dataloader</a> class, we will make use of the <code>collate_fn</code> argument. For more details, check out the <code>collate</code> function in <code>src/tripadvisor_dataset</code>.\n","\n","In addition, we use the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\" target=\"_blank\">pad_sequence</a> that pads shorter sequences with 0.\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":200,"status":"ok","timestamp":1633959211511,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"vdJHG3TS6RbE","outputId":"47863982-dcf8-4b1e-d450-6b932a1daaf7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data: \n"," tensor([[2552,  120,  176],\n","        [  17,   84,  158],\n","        [ 963,    7, 6022],\n","        ...,\n","        [  77,    0,    0],\n","        [ 520,    0,    0],\n","        [ 571,    0,    0]])\n","\n","Labels: \n"," tensor([1., 2., 1.])\n","\n","Sequence Lengths: \n"," tensor([1863, 1673, 1381])\n","\n","\n"]}],"source":["loader = DataLoader(train_dataset, batch_size=3, collate_fn=collate)\n","for batch in loader:\n","    print('Data: \\n', batch['data'])\n","    print('\\nLabels: \\n', batch['label'])\n","    print('\\nSequence Lengths: \\n', batch['lengths'])\n","    print('\\n')\n","    break"]},{"cell_type":"markdown","metadata":{"id":"lVjQu0gd6RbF"},"source":["## 2. Pre-trained GloVe embeddings\n","\n","Rather than training our own word vectors from scratch, we can also leverage GloVe embeddings. Its authors have released four text files with word vectors trained on different massive web datasets. They are available for download [here](https://nlp.stanford.edu/projects/glove/). We will use `Wikipedia 2014 + Gigaword 5` which is the smallest file ([`glove.6B.zip`](http://nlp.stanford.edu/data/wordvecs/glove.6B.zip)) with 822 MB. It was trained on a corpus of 6 billion tokens and contains a vocabulary of 400 thousand tokens.\n","\n","First, download `glove.6B.zip` to the `data/` folder. After unzipping the downloaded file we find four txt files: `glove.6B.50d.txt`, `glove.6B.100d.txt`, `glove.6B.200d.txt`, `glove.6B.300d.txt`. As their filenames suggests, they have vectors with different dimensions. We pick the smallest one with words represented by vectors of dim 50 (`glove.6B.50d.txt`).\n","\n","Given that the vocabulary have 400k tokens, we will use `bcolz` to store the array of vectors. It provides columnar, chunked data containers that can be compressed either in-memory and on-disk. It is based on `NumPy`, and uses it as the standard data container to communicate with `bcolz` objects.\n","\n","We need to parse the file to get as output: list of words, dictionary mapping each word to their id (position) and array of vectors. We then save the outputs to disk for future uses ( with the `parse_glove()` function). Using those objects we can now create a dictionary that given a word returns its vector. GloVe’s vocabulary is likely to be different from our dataset’s vocabulary. For each word in dataset’s vocabulary, we check if it is GloVe’s vocabulary. If it is, we load its pre-trained word vector. Otherwise, we initialize a random vector."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":242184,"status":"ok","timestamp":1633959453692,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"oFeqXVI-6RbF","outputId":"6053ee34-c78e-4143-a93c-c073b812577a"},"outputs":[{"name":"stdout","output_type":"stream","text":["17242 words have been found in GloVe. For the remaining 2760 words a random vector has been assigned.\n"]}],"source":["if params['pretrained_embedding'] is not None:\n","    glove_path = os.path.join(os.path.abspath(os.getcwd()), \"data\", \"glove.6B\")\n","    \n","    # uncomment this the first time you use GloVe embeddings.\n","    # parse_glove(glove_path=glove_path, embedding_dim=params['pretrained_embedding'])\n","\n","    vectors = bcolz.open(f'{glove_path}/6B.{params[\"pretrained_embedding\"]}.dat')[:]\n","    words = pickle.load(open(f'{glove_path}/6B.{params[\"pretrained_embedding\"]}_words.pkl', 'rb'))\n","    word2idx = pickle.load(open(f'{glove_path}/6B.{params[\"pretrained_embedding\"]}_idx.pkl', 'rb'))\n","\n","    glove = {w: vectors[word2idx[w]] for w in words}\n","\n","    matrix_len = len(vocab)\n","    embedding_weights = np.zeros((matrix_len, params['pretrained_embedding']))\n","    scale = np.std(vectors, axis=0)\n","    words_found = 0\n","    for i, word in enumerate(vocab):\n","        try: \n","            embedding_weights[i] = glove[word]\n","            words_found += 1\n","        except KeyError:\n","            embedding_weights[i] = np.random.normal(scale=scale, size=(params['pretrained_embedding'], ))\n","    print(f\"{words_found} words have been found in GloVe. For the remaining {matrix_len - words_found} words a random vector has been assigned.\")\n","    embedding_weights = torch.from_numpy(embedding_weights)\n","else:\n","    embedding_weights = None\n"]},{"cell_type":"markdown","metadata":{"id":"b7u4zQNk6RbG"},"source":["## 3. Creating a Sentiment Classifier\n","\n","After we have loaded the data, it is time to define a model and start training and testing.\n","\n","### Evaluation Metrics\n","\n","Since we need to predict positive, neutral or negative, we use `cross-entropy loss` to train our model. \n","\n","We usually take *accuracy* as our metric for most classification problems, however, ratings are ordered, *RMSE* (root mean squared error) is a reasonable alternative. "]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":14606,"status":"ok","timestamp":1633959468288,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"jikg-BT86RbH"},"outputs":[],"source":["if params['weighted_loss']:\n","    loss_weights = 1/torch.Tensor(list(calc_distr(train_data).values())).cuda()\n","else:\n","    loss_weights = None\n","\n","loss_fn = nn.CrossEntropyLoss(weight=loss_weights)\n","\n","@torch.no_grad()\n","def validation_metrics (model, data_loader):\n","    model.eval()\n","    correct = np.array([0]*params['output_size'])\n","    total = np.array([0]*params['output_size'])\n","    sum_loss = 0.0\n","    sum_rmse = 0.0\n","    device = next(model.parameters()).device\n","    preds = []\n","\n","    for i, x in enumerate(data_loader):\n","        input = x['data'].to(device)\n","        lengths = x['lengths']\n","        label = x['label'].to(device)\n","        \n","        pred = model(input, lengths)\n","        loss = loss_fn(pred, label.long())\n","        pred = torch.max(pred, 1)[1]\n","        preds.extend(pred.cpu().tolist())\n","\n","        for i, y in enumerate(label.int().cpu()):\n","            correct[y.item()] += (pred[i].cpu() == y).float()\n","            total[y.item()] += 1\n","        sum_loss += loss.item()*label.shape[0]\n","        sum_rmse += np.sqrt(mean_squared_error(pred.cpu(), label.unsqueeze(-1).cpu()))*label.shape[0]\n","    \n","    class_acc = correct / total\n","    return sum_loss/total.sum(), np.mean(class_acc), sum_rmse/total.sum(), preds"]},{"cell_type":"markdown","metadata":{"id":"I-W6HCu16RbH"},"source":["### Design the model\n","\n","See <code>src/text_classifiers.py</code> for the implementation of the <code>RNNClassifier</code>."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3429,"status":"ok","timestamp":1633959471698,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"dPxlmIrX6RbI","outputId":"f599a958-a6c5-4450-8cfb-29d8c51d5a0d"},"outputs":[{"data":{"text/plain":["RNNClassifier(\n","  (embedding): Embedding(20002, 300, padding_idx=0)\n","  (rnn): LSTM(300, 512, num_layers=5, dropout=0.5, bidirectional=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (fc): Linear(in_features=1024, out_features=3, bias=True)\n",")"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["model = RNNClassifier(num_embeddings=len(vocab), embedding_dim=params['embedding_dim'], hidden_size=params['hidden_size'], rnn_type=params['rnn_type'], \n","                    bidirectional=params['bidirectional'], n_layers = params['n_layers'], dropout = params['dropout'], output_size= params['output_size'],\n","                    pretrained_embedding=embedding_weights)\n","model.apply(initialize_weights)"]},{"cell_type":"markdown","metadata":{"id":"MH55Imes6RbI"},"source":["### Train the model\n","\n","Note the **collate function** used with the `DataLoader`."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1959,"status":"ok","timestamp":1633959473639,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"n1LrI0nm6RbJ","outputId":"26f90f6f-c82e-493a-9835-6330c2a940b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda...\n","\n"]}],"source":["# Training configs\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print('Using {}...\\n'.format(device))\n","\n","# set up tensorboard storage\n","file_name = f\"{params['rnn_type']}_{params['embedding_dim']}_{params['hidden_size']}_{params['n_layers']}_{params['lr']}\"\n","file_name += \"_bidir\" if params[\"bidirectional\"] else \"\"\n","file_name += f'_glove{params[\"pretrained_embedding\"]}' if (params[\"pretrained_embedding\"] is not None) else \"\"\n","dir_name = os.path.join(os.path.abspath(os.getcwd()),\"tensorboard_log\", file_name)\n","writer = SummaryWriter(log_dir=dir_name) \n","\n","# Dataloaders, note the collate function\n","train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], collate_fn=collate, drop_last=True, num_workers=2, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], collate_fn=collate, drop_last=False, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"LFNHH-sL6RbJ"},"source":["Visualize model in tensorboard."]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1633959473640,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"TqT7D3p26RbJ"},"outputs":[],"source":["# x = next(iter(train_loader))\n","# writer.add_graph(model=model.cpu(), input_to_model=(x['data'], x['lengths']))\n","# writer.close()"]},{"cell_type":"markdown","metadata":{"id":"1xdJd4cn6RbJ"},"source":["Training loop:"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9371780,"status":"ok","timestamp":1633968845417,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"AyW016XO6RbK","outputId":"188b081f-ba58-409b-c324-9b65a0d9ae5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1/100... Train Loss: 0.856... Val Loss: 0.678... Val Acc: 0.686... Val RMSE: 0.649\n","Epoch: 2/100... Train Loss: 0.714... Val Loss: 0.717... Val Acc: 0.670... Val RMSE: 0.617\n","Epoch: 3/100... Train Loss: 0.661... Val Loss: 0.619... Val Acc: 0.712... Val RMSE: 0.607\n","Epoch: 4/100... Train Loss: 0.640... Val Loss: 0.584... Val Acc: 0.735... Val RMSE: 0.596\n","Epoch: 5/100... Train Loss: 0.614... Val Loss: 0.610... Val Acc: 0.720... Val RMSE: 0.594\n","Epoch: 6/100... Train Loss: 0.599... Val Loss: 0.612... Val Acc: 0.725... Val RMSE: 0.604\n","Epoch: 7/100... Train Loss: 0.582... Val Loss: 0.566... Val Acc: 0.741... Val RMSE: 0.551\n","Epoch: 8/100... Train Loss: 0.562... Val Loss: 0.588... Val Acc: 0.734... Val RMSE: 0.572\n","Epoch: 9/100... Train Loss: 0.551... Val Loss: 0.578... Val Acc: 0.745... Val RMSE: 0.565\n","Epoch: 10/100... Train Loss: 0.532... Val Loss: 0.552... Val Acc: 0.750... Val RMSE: 0.564\n","Epoch: 11/100... Train Loss: 0.526... Val Loss: 0.579... Val Acc: 0.737... Val RMSE: 0.564\n","Epoch: 12/100... Train Loss: 0.517... Val Loss: 0.556... Val Acc: 0.756... Val RMSE: 0.539\n","Epoch: 13/100... Train Loss: 0.493... Val Loss: 0.628... Val Acc: 0.718... Val RMSE: 0.545\n","Epoch: 14/100... Train Loss: 0.481... Val Loss: 0.601... Val Acc: 0.733... Val RMSE: 0.551\n","Epoch: 15/100... Train Loss: 0.464... Val Loss: 0.595... Val Acc: 0.743... Val RMSE: 0.558\n","Epoch: 16/100... Train Loss: 0.455... Val Loss: 0.573... Val Acc: 0.752... Val RMSE: 0.554\n","Epoch: 17/100... Train Loss: 0.412... Val Loss: 0.619... Val Acc: 0.757... Val RMSE: 0.528\n","Epoch: 18/100... Train Loss: 0.403... Val Loss: 0.596... Val Acc: 0.753... Val RMSE: 0.533\n","Epoch: 19/100... Train Loss: 0.382... Val Loss: 0.603... Val Acc: 0.754... Val RMSE: 0.546\n","Epoch: 20/100... Train Loss: 0.373... Val Loss: 0.638... Val Acc: 0.738... Val RMSE: 0.548\n","Epoch: 21/100... Train Loss: 0.350... Val Loss: 0.693... Val Acc: 0.740... Val RMSE: 0.536\n","Epoch: 22/100... Train Loss: 0.341... Val Loss: 0.713... Val Acc: 0.733... Val RMSE: 0.539\n","Epoch: 23/100... Train Loss: 0.312... Val Loss: 0.717... Val Acc: 0.738... Val RMSE: 0.543\n","Epoch: 24/100... Train Loss: 0.299... Val Loss: 0.762... Val Acc: 0.725... Val RMSE: 0.546\n","Epoch: 25/100... Train Loss: 0.291... Val Loss: 0.750... Val Acc: 0.739... Val RMSE: 0.550\n","Epoch: 26/100... Train Loss: 0.286... Val Loss: 0.752... Val Acc: 0.722... Val RMSE: 0.557\n","Epoch: 27/100... Train Loss: 0.269... Val Loss: 0.791... Val Acc: 0.735... Val RMSE: 0.545\n","Early stopping at epoch 17.\n"]}],"source":["# Move model to the device we are using\n","model = model.to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n","# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=params['lr_decay'])\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=np.floor(params['patience']/2), factor=params['lr_decay'])\n","\n","train_loss_history = []\n","val_loss_history = []\n","best_acc = 0\n","model.train()\n","\n","# train for some number of epochs\n","for e in range(params['epochs']):\n","    model.train()\n","    sum_loss = 0.0\n","    total = 0\n","\n","    # batch loop\n","    for i, x in enumerate(train_loader):\n","        inputs = x['data'].to(device)\n","        lengths = x['lengths']\n","        labels = x['label'].to(device)\n","        # zero accumulated gradients\n","        optimizer.zero_grad()\n","\n","        # get the output from the model\n","        output = model(inputs, lengths)\n","\n","        # calculate the loss and perform backprop\n","        loss = loss_fn(output, labels.long())\n","        loss.backward()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        clip_grad_norm_(model.parameters(), max_norm=params['gclip'])\n","        optimizer.step()\n","\n","        sum_loss += loss.item()*x['label'].shape[0]\n","        total += x['label'].shape[0]\n","               \n","        # loss stats\n","        if (params['print_every'] is not None) and (i % params['print_every'] == 0):\n","            print('Step {} / {}, Loss {}'.format(i, len(train_loader), loss.item()))\n","    # Get validation loss\n","    val_loss, val_acc, val_rmse, _ = validation_metrics(model, val_loader)\n","\n","    # Learning rate scheduler\n","    scheduler.step(val_loss)\n","    \n","    writer.add_scalar(\"Train Loss\", sum_loss/total, e)\n","    writer.add_scalar(\"Validation Loss\", val_loss, e)\n","    writer.add_scalar(\"Validation Accuracy\", val_acc, e)\n","    writer.add_scalar(\"Validation RMSE\", val_rmse, e)\n","\n","    train_loss_history.append(sum_loss/total)\n","    val_loss_history.append(val_loss)\n","\n","    print(\"Epoch: {}/{}...\".format(e+1, params['epochs']),\n","            \"Train Loss: {:.3f}...\".format(sum_loss/total),\n","            \"Val Loss: {:.3f}...\".format(val_loss),\n","            \"Val Acc: {:.3f}...\".format(val_acc),\n","            \"Val RMSE: {:.3f}\".format(val_rmse))\n","    # Early stopping\n","    if val_acc > best_acc:\n","        pat = 0\n","        best_acc = val_acc\n","        best_model_weights = copy.deepcopy(model.state_dict())\n","        best_epoch = e+1\n","    else:\n","        pat += 1\n","\n","    if pat == params['patience']:\n","        print('Early stopping at epoch {epoch}.'.format(epoch=best_epoch))\n","        break\n","            \n","model.load_state_dict(best_model_weights)\n","torch.save(model.state_dict(), os.path.join(os.path.abspath(os.getcwd()), \"saved_models\", file_name+\".pth\"))"]},{"cell_type":"markdown","metadata":{"id":"Fxm3mRKq6RbK"},"source":["Open Tensorboard GUI to see the logs:"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1633968845419,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"ZTWXDdvd6RbK"},"outputs":[],"source":["# %load_ext tensorboard\n","# %tensorboard --logdir ./tensorboard_log"]},{"cell_type":"markdown","metadata":{"id":"1U0sZxex6RbL"},"source":["Let's plot the loss curves in one figure to have an insight of the training. "]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":214},"executionInfo":{"elapsed":664,"status":"ok","timestamp":1633968846080,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"LouugqRF6RbL","outputId":"a6c00791-a11c-4a4b-c670-5c694c446c88"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 288x216 with 1 Axes>"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["fig = plt.figure()\n","# plt.title('Loss curves')\n","plt.plot(train_loss_history, '-', label='train')\n","plt.plot(val_loss_history, '-', label='val')\n","idx = len(train_loss_history)- params['patience']\n","plt.plot(idx, val_loss_history[idx], color='green', marker='o', markersize=6)\n","# plt.ylim([0.2, 0.9])\n","plt.legend(loc='lower right')\n","# plt.xlabel('Iteration')\n","writer.add_figure(f\"Loss curves\", fig)\n","fig"]},{"cell_type":"markdown","metadata":{"id":"_rjwvQXA6RbM"},"source":["## Testing the Model\n","\n","As we trained a model and improved it on the validation set, we can now test it on the test set."]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13819,"status":"ok","timestamp":1633968859896,"user":{"displayName":"Márton Szép","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01518699844804606877"},"user_tz":-120},"id":"4-wtnbt76RbM","outputId":"52988800-412f-4e1a-de30-78beeb1232bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Macro Accuracy: 0.7452321412503057, RMSE: 0.5393696224299861\n","              precision    recall  f1-score   support\n","\n","    negative       0.75      0.81      0.78       643\n","     neutral       0.69      0.64      0.66      1644\n","    positive       0.76      0.79      0.77      1811\n","\n","    accuracy                           0.73      4098\n","   macro avg       0.73      0.75      0.74      4098\n","weighted avg       0.73      0.73      0.73      4098\n","\n"]}],"source":["test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], collate_fn=collate, drop_last=False, num_workers=2)\n","\n","_, test_acc, test_rmse, y_pred = validation_metrics(model, test_loader)\n","y_true = [x['label'].item() for x in test_dataset]\n","report = classification_report(y_true, y_pred, target_names=['negative', 'neutral', 'positive'], output_dict=True)\n","print(f\"Macro Accuracy: {test_acc}, RMSE: {test_rmse}\")\n","print(classification_report(y_true, y_pred, target_names=['negative', 'neutral', 'positive']))\n","\n","writer.add_hparams(params,{\"macro_acc\":test_acc, \"acc\": report['accuracy'], \"rmse\": test_rmse, \"f1-score\": report['macro avg']['f1-score'],\n","                         \"precision\":report['macro avg']['precision'], \"recall\":report['macro avg']['recall']})\n","writer.close()"]},{"cell_type":"markdown","metadata":{"id":"fS7xA-416RbM"},"source":["## Inference\n","\n","Now that we trained a sufficiently good sentiment classifier, let's run the below cell and type some text to see some predictions (type exit to quit the demo). Since we used a small data, don't expect too much :)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0mK3UNI6RbN","outputId":"96cb1e60-ff27-46c7-a880-b2d9d2a50c9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input text: awesome but expensive\n"," Sentiment -> :), Confidence -> 71.865% \n","\n","Input text: expensive but awesome\n"," Sentiment -> :/, Confidence -> 42.852% \n","\n","Input text: comfortable place polite staff\n"," Sentiment -> :/, Confidence -> 75.686% \n","\n","Input text: loved it rooms could have been cleaner but food was great\n"," Sentiment -> :), Confidence -> 84.820% \n","\n","Input text: loved it\n"," Sentiment -> :), Confidence -> 84.636% \n","\n","Input text: disgusting rooms, unbearable smell\n"," Sentiment -> :(, Confidence -> 86.970% \n","\n","Input text: totally worth the money, staff helpful, had a great time\n"," Sentiment -> :), Confidence -> 83.638% \n","\n","Input text: kids loved it, great place to rest\n"," Sentiment -> :), Confidence -> 81.685% \n","\n"]}],"source":["text = ''\n","w2i = vocab\n","while True:\n","    text = input()\n","    if text == 'exit':\n","        break\n","    \n","    words = torch.tensor([\n","        w2i.get(word, w2i['<unk>'])\n","        for word in nltk.word_tokenize(text)\n","    ]).long().to(device).view(-1, 1)  # T x B\n","    with torch.no_grad():\n","        pred = model(words).cpu()\n","        probs = F.softmax(pred, dim=1)\n","    prob, idx = torch.max(probs, dim=1)\n","    if idx == 0:\n","        txt = ':('\n","    elif idx == 1:\n","        txt = ':/'\n","    elif idx == 2:\n","        txt = ':)'\n","        \n","    print('Input text: {}\\n'.format(text),\n","        'Sentiment -> {}, Confidence -> {:.3f}% '.format(txt, *prob*100))\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXiKlwsD6RbN"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"2_SentimentAnalysis_PyTorch.ipynb","provenance":[],"toc_visible":true},"interpreter":{"hash":"2c14af1f25c15f29859e500cefbe6a80672bae49cca61ec75a6583202a38fbf3"},"kernelspec":{"display_name":"Python 3.7.11 64-bit ('nlp': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"}},"nbformat":4,"nbformat_minor":0}
