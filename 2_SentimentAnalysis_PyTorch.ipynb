{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('nlp': conda)"
  },
  "interpreter": {
   "hash": "c0a2f5f2ec388a4b45beb4ef144deae29e8f9d03c06f4c2bd14f758dcb12937e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis 2 - PyTorch\n",
    "\n",
    "## 0. Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# General packages\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "import bcolz\n",
    "import pickle\n",
    "\n",
    "# NLP packages\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Modelling packages\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from src.text_classifiers import RNNClassifier, initialize_weights\n",
    "from src.tripadvisor_dataset import SentimentDataset, collate\n",
    "from src.utils import calc_distr, parse_glove\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "params={\n",
    "    'SEED': 123,            # seed for random processes                 \n",
    "    'max_features': 10000,  # vocabulary size\n",
    "    'embedding_dim': 128,   # size of an embedding vector (if not using pretrained embedding vectors)\n",
    "    'hidden_size': 256,     # number of features in the hidden state of the rnn layer\n",
    "    'rnn_type': 'gru',      # use 'rnn', 'lstm' or 'gru'\n",
    "    'bidirectional': True,  # If True, use bidirectional rnn \n",
    "    'n_layers': 5,          # number of layers in the stacked rnn \n",
    "    'dropout': 0.5,         # if non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer\n",
    "    'output_size': 3,       # number of classes\n",
    "    'gclip': 40,            # To tackle the exploding gradient problem, set gclip and use clip_grad_norm_(X, gclip)\n",
    "    'batch_size': 16,       # Batch size for stochastic gradient descent\n",
    "    'lr': 5e-4,             # Learning rate for the Adam optimizer\n",
    "    'lr_decay': 0.95,       # Learning rate decay parameter for exponential lr scheduling\n",
    "    'epochs': 35,           # Number of epochs to train the model\n",
    "    'print_every': None,    # Frequency of printing the loss in an epoch\n",
    "    'patience': 5,          # Patience value used for early stopping if the performance does not increase.\n",
    "    'weighted_loss': True,  # Use a weighted Cross Entropy Loss function to balance classes\n",
    "    'pretrained_embedding': True    # Use pretrained embedding vectors, otherwise train from scratch\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Dataset\n",
    "### Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data_root = os.path.join(os.path.abspath(os.getcwd()), \"data\", \"tripadvisor_hotel_reviews_preproc.csv\")\n",
    "data = pd.read_csv(data_root)\n",
    "data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review_preprocessed</th>\n",
       "      <th>Sentiment_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>4</td>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>2</td>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
       "      <td>3</td>\n",
       "      <td>nice room experience hotel monaco seattle good...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
       "      <td>5</td>\n",
       "      <td>unique great stay wonderful time hotel monaco ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay, went seahawk game aweso...</td>\n",
       "      <td>5</td>\n",
       "      <td>great stay great stay went seahawk game awesom...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating  \\\n",
       "0  nice hotel expensive parking got good deal sta...       4   \n",
       "1  ok nothing special charge diamond member hilto...       2   \n",
       "2  nice rooms not 4* experience hotel monaco seat...       3   \n",
       "3  unique, great stay, wonderful time hotel monac...       5   \n",
       "4  great stay great stay, went seahawk game aweso...       5   \n",
       "\n",
       "                                 Review_preprocessed  Sentiment_rating  \n",
       "0  nice hotel expensive parking got good deal sta...                 1  \n",
       "1  ok nothing special charge diamond member hilto...                 0  \n",
       "2  nice room experience hotel monaco seattle good...                 1  \n",
       "3  unique great stay wonderful time hotel monaco ...                 2  \n",
       "4  great stay great stay went seahawk game awesom...                 2  "
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Vocabulary\n",
    "\n",
    "Build a vocabulary that only consider the top `max_features` ordered by term frequency across the corpus."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "tfidfconverter = TfidfVectorizer(use_idf=True, max_features=params['max_features'], min_df=0, max_df=1.0)\n",
    "tfidfconverter.fit_transform(data['Review_preprocessed'])\n",
    "pre_vocab = tfidfconverter.vocabulary_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print('Vocabulary size:', len(pre_vocab), '\\n\\n  Sample words\\n{}'.format('-' * 20))\n",
    "sample_words = random.sample(list(pre_vocab.keys()), 10)\n",
    "for word in sample_words:\n",
    "    print(' {}'.format(word))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary size: 10000 \n",
      "\n",
      "  Sample words\n",
      "--------------------\n",
      " violent\n",
      " freezing\n",
      " buffet\n",
      " private\n",
      " transat\n",
      " gratis\n",
      " paid\n",
      " bldg\n",
      " briefly\n",
      " philadelphia\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In addition to the words that appear in our data, we need to have two special words:\n",
    "\n",
    "- `<eos>` End of sequence symbol used for padding\n",
    "- `<unk>` Words unknown in our vocabulary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "vocab = {'<eos>': 0, '<unk>': 1}\n",
    "for key in pre_vocab.keys():\n",
    "    vocab[key] = len(vocab)\n",
    "print('Vocabulary size:', len(vocab))\n",
    "# for i in range(len(vocab)-50, len(vocab)):\n",
    "#     print(list(vocab.keys())[list(vocab.values()).index(i)])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary size: 10002\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Dataset\n",
    "\n",
    "Steps:\n",
    "* Tokenize data\n",
    "* Create index-label pairs based on vocabulary\n",
    "* Divide into train, validation and test sets\n",
    "* Wrapping to PyTorch dataset\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "tokenized_data = []\n",
    "for i in range(len(data)):\n",
    "    tokenized_data.append((nltk.word_tokenize(data['Review_preprocessed'][i]), data['Sentiment_rating'][i]))\n",
    "print(tokenized_data[-1])    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(['people', 'talking', 'ca', 'believe', 'excellent', 'rating', 'hotel', 'yes', 'patricia', 'extremely', 'helpful', 'fluent', 'language', 'go', 'way', 'make', 'welcome', 'said', 'place', 'bit', 'dump', 'inexpensive', 'hotel', 'expensive', 'city', 'place', 'bit', 'dated', 'institutional', 'odor', 'charm', 'funeral', 'home', 'walking', 'step', 'hotel', 'girlfriend', 'step', 'condom', 'yes', 'condom', 'step', 'lot', 'guy', 'hanging', 'desk', 'hallway', 'girlfriend', 'swears', 'house', 'prostitution', 'patricia', 'arrange', 'taxi', 'following', 'morning', 'stayed', 'night', 'wrong', 'information', 'cost', 'fare', 'room', 'clean', 'large', 'bathroom', 'small', 'passable', 'night', 'glad', 'leave', 'following', 'morning', 'recommend', 'extended', 'stay', 'unless', 'tight', 'budget', 'care', 'look', 'feel', 'place'], 0)\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "indexed_data = []\n",
    "for tokens, label in tokenized_data:\n",
    "    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]    \n",
    "    # the token that is not in vocab get assigned <unk>\n",
    "    indexed_data.append((indices, label))\n",
    "print(indexed_data[-1])    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "([49, 50, 625, 1353, 321, 1267, 3, 936, 8650, 770, 148, 8227, 385, 1578, 282, 175, 469, 98, 440, 937, 2036, 2679, 3, 4, 388, 440, 937, 2633, 1, 2023, 587, 1, 559, 66, 1154, 3, 380, 1154, 5643, 936, 5643, 1154, 541, 2008, 1305, 96, 51, 380, 1, 428, 1, 8650, 250, 368, 1001, 42, 447, 41, 875, 1669, 783, 1059, 27, 28, 217, 92, 538, 5590, 41, 2499, 409, 1001, 42, 441, 2098, 9, 827, 1958, 1301, 815, 295, 357, 440], 0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataset class also reverse sorts the sequences with respect to the lengths. Thanks to this sorting, we can reduce the total number of padded elements, which means that we have less computations for padded values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "combined_data = [(raw_text, tokens, indices, label) for (raw_text, label), (tokens, _), (indices, _)\n",
    "    in zip(list(data[['Review_preprocessed', 'Sentiment_rating']].to_records(index=False)), tokenized_data, indexed_data)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's divide the dataset into train, validation and test sets (60%-20%-20%). Stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "val_test_len = int(len(combined_data) * 0.2)\n",
    "train_val_data, test_data = train_test_split(combined_data,test_size=val_test_len, random_state=params['SEED'], stratify=data['Sentiment_rating'])\n",
    "df = pd.DataFrame(train_val_data, columns=['raw_text', 'tokens', 'indices', 'label'])\n",
    "train_data, val_data = train_test_split(train_val_data,test_size=val_test_len, random_state=params['SEED'], stratify=df['label'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check out the distribution of the classes in the created datasets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(calc_distr(combined_data))\n",
    "print(calc_distr(train_val_data))\n",
    "print(calc_distr(train_data))\n",
    "print(calc_distr(val_data))\n",
    "print(calc_distr(test_data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'0': 0.15684934849446097, '1': 0.4012981308867308, '2': 0.44185252061880825}\n",
      "{'0': 0.15683523455133289, '1': 0.4013298359055695, '2': 0.44183492954309767}\n",
      "{'0': 0.15681171207808053, '1': 0.40130134200894674, '2': 0.44188694591297273}\n",
      "{'0': 0.15690580771107857, '1': 0.40141532454856027, '2': 0.44167886774036114}\n",
      "{'0': 0.15690580771107857, '1': 0.4011713030746706, '2': 0.4419228892142509}\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Check out the number of data samples in the created datasets.\n",
    "print(len(combined_data))\n",
    "print(len(train_val_data))\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20491\n",
      "16393\n",
      "12295\n",
      "4098\n",
      "4098\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us wrap our data in a PyTorch dataset. For more details, check out the previous notebook and the corresponding dataset class defined in `src/tripadvisor_dataset.py`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#Define a Dataset Class for train, val and test set\n",
    "train_dataset = SentimentDataset(train_data)\n",
    "val_dataset = SentimentDataset(val_data)\n",
    "test_dataset = SentimentDataset(test_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that in the dataset we created, not all sequences have the same length. Therefore, we cannot minibatch the data trivially. This means we cannot use a `DataLoader` class easily.\n",
    "\n",
    "To solve the problem, we need to pad the sequences with <code> <eos> </code> tokens that we indexed as zero. To integrate this approach into the Pytorch <a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\" target=\"_blank\">Dataloader</a> class, we will make use of the <code>collate_fn</code> argument. For more details, check out the <code>collate</code> function in <code>src/tripadvisor_dataset</code>.\n",
    "\n",
    "In addition, we use the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\" target=\"_blank\">pad_sequence</a> that pads shorter sequences with 0.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "loader = DataLoader(train_dataset, batch_size=3, collate_fn=collate)\n",
    "for batch in loader:\n",
    "    print('Data: \\n', batch['data'])\n",
    "    print('\\nLabels: \\n', batch['label'])\n",
    "    print('\\nSequence Lengths: \\n', batch['lengths'])\n",
    "    print('\\n')\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data: \n",
      " tensor([[2399,  120,  174],\n",
      "        [  17,   84,  156],\n",
      "        [ 930,    7, 5148],\n",
      "        ...,\n",
      "        [  77,    0,    0],\n",
      "        [ 510,    0,    0],\n",
      "        [ 559,    0,    0]])\n",
      "\n",
      "Labels: \n",
      " tensor([1., 2., 1.])\n",
      "\n",
      "Sequence Lengths: \n",
      " tensor([1863, 1673, 1381])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Pre-trained GloVe embeddings\n",
    "\n",
    "Rather than training our own word vectors from scratch, we can also leverage GloVe embeddings. Its authors have released four text files with word vectors trained on different massive web datasets. They are available for download [here](https://nlp.stanford.edu/projects/glove/). We will use `Wikipedia 2014 + Gigaword 5` which is the smallest file ([`glove.6B.zip`](http://nlp.stanford.edu/data/wordvecs/glove.6B.zip)) with 822 MB. It was trained on a corpus of 6 billion tokens and contains a vocabulary of 400 thousand tokens.\n",
    "\n",
    "First, download `glove.6B.zip` to the `data/` folder. After unzipping the downloaded file we find four txt files: `glove.6B.50d.txt`, `glove.6B.100d.txt`, `glove.6B.200d.txt`, `glove.6B.300d.txt`. As their filenames suggests, they have vectors with different dimensions. We pick the smallest one with words represented by vectors of dim 50 (`glove.6B.50d.txt`).\n",
    "\n",
    "Given that the vocabulary have 400k tokens, we will use `bcolz` to store the array of vectors. It provides columnar, chunked data containers that can be compressed either in-memory and on-disk. It is based on `NumPy`, and uses it as the standard data container to communicate with `bcolz` objects.\n",
    "\n",
    "We need to parse the file to get as output: list of words, dictionary mapping each word to their id (position) and array of vectors. We then save the outputs to disk for future uses ( with the `parse_glove()` function). Using those objects we can now create a dictionary that given a word returns its vector. GloVe’s vocabulary is likely to be different from our dataset’s vocabulary. For each word in dataset’s vocabulary, we check if it is GloVe’s vocabulary. If it is, we load its pre-trained word vector. Otherwise, we initialize a random vector."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "if params['pretrained_embedding']:\n",
    "    glove_path = os.path.join(os.path.abspath(os.getcwd()), \"data\", \"glove.6B\")\n",
    "    emb_dim = 50 # could also use 100, 200 or 300\n",
    "\n",
    "    # uncomment this the first time you use GloVe embeddings.\n",
    "    # parse_glove(glove_path=glove_path, embedding_dim=emb_dim)\n",
    "\n",
    "    vectors = bcolz.open(f'{glove_path}/6B.{emb_dim}.dat')[:]\n",
    "    words = pickle.load(open(f'{glove_path}/6B.{emb_dim}_words.pkl', 'rb'))\n",
    "    word2idx = pickle.load(open(f'{glove_path}/6B.{emb_dim}_idx.pkl', 'rb'))\n",
    "\n",
    "    glove = {w: vectors[word2idx[w]] for w in words}\n",
    "\n",
    "    matrix_len = len(vocab)\n",
    "    embedding_weights = np.zeros((matrix_len, emb_dim))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(vocab):\n",
    "        try: \n",
    "            embedding_weights[i] = glove[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            embedding_weights[i] = np.random.normal(scale=np.std(vectors, axis=0), size=(emb_dim, ))\n",
    "    print(f\"{words_found} words have been found in GloVe. For the remaining {matrix_len - words_found} words a random vector has been assigned.\")\n",
    "else:\n",
    "    embedding_weights = None\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9523 words have been found in GloVe. For the remaining 479 words a random vector has been assigned.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Creating a Sentiment Classifier\n",
    "\n",
    "After we have loaded the data, it is time to define a model and start training and testing.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "Since we need to predict positive, neutral or negative, we use `cross-entropy loss` to train our model. \n",
    "\n",
    "We usually take *accuracy* as our metric for most classification problems, however, ratings are ordered, *RMSE* (root mean squared error) is a reasonable alternative. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "if params['weighted_loss']:\n",
    "    loss_weights = 1/torch.Tensor(list(calc_distr(train_data).values())).cuda()\n",
    "else:\n",
    "    loss_weights = None\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=loss_weights)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_metrics (model, data_loader):\n",
    "    model.eval()\n",
    "    correct = np.array([0]*params['output_size'])\n",
    "    total = np.array([0]*params['output_size'])\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    device = next(model.parameters()).device\n",
    "    preds = []\n",
    "\n",
    "    for i, x in enumerate(data_loader):\n",
    "        input = x['data'].to(device)\n",
    "        lengths = x['lengths']\n",
    "        label = x['label'].to(device)\n",
    "        \n",
    "        pred = model(input, lengths)\n",
    "        loss = loss_fn(pred, label.long())\n",
    "        pred = torch.max(pred, 1)[1]\n",
    "        preds.extend(pred.cpu().tolist())\n",
    "\n",
    "        for i, y in enumerate(label.int().cpu()):\n",
    "            correct[y.item()] += (pred[i].cpu() == y).float()\n",
    "            total[y.item()] += 1\n",
    "        sum_loss += loss.item()*label.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred.cpu(), label.unsqueeze(-1).cpu()))*label.shape[0]\n",
    "    \n",
    "    class_acc = correct / total\n",
    "    return sum_loss/total.sum(), np.mean(class_acc), sum_rmse/total.sum(), preds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Design the model\n",
    "\n",
    "See <code>src/text_classifiers.py</code> for the implementation of the <code>RNNClassifier</code>."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "model = RNNClassifier(num_embeddings=len(vocab), embedding_dim=params['embedding_dim'], hidden_size=params['hidden_size'], rnn_type=params['rnn_type'], \n",
    "                    bidirectional=params['bidirectional'], n_layers = params['n_layers'], dropout = params['dropout'], output_size= params['output_size'],\n",
    "                    pretrained_embedding=torch.from_numpy(embedding_weights))\n",
    "model.apply(initialize_weights)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embedding): Embedding(10002, 50, padding_idx=0)\n",
       "  (rnn): GRU(50, 256, num_layers=5, dropout=0.4, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RNNClassifier(\n",
      "  (embedding): Embedding(10002, 50, padding_idx=0)\n",
      "  (rnn): GRU(50, 256, num_layers=5, dropout=0.4, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the model\n",
    "\n",
    "Note the **collate function** used with the `DataLoader`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# Training configs\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Using {}...\\n'.format(device))\n",
    "\n",
    "# set up tensorboard storage\n",
    "dir_name = f\"{params['rnn_type']}_{params['embedding_dim']}_{params['hidden_size']}_{params['n_layers']}_{params['lr']}\"\n",
    "dir_name += \"_bidir\" if params[\"bidirectional\"] else \"\"\n",
    "dir_name += \"_glove\" if params[\"pretrained_embedding\"] else \"\"\n",
    "writer = SummaryWriter(f\"tensorboard_log/{dir_name}/\") \n",
    "\n",
    "# Dataloaders, note the collate function\n",
    "train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], collate_fn=collate, drop_last=True, num_workers=8, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], collate_fn=collate, drop_last=False, num_workers=8)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda...\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize model in tensorboard."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "x = next(iter(train_loader))\n",
    "writer.add_graph(model=model.cpu(), input_to_model=(x['data'], x['lengths']))\n",
    "writer.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training loop:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# Move model to the device we are using\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=params['lr_decay'])\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "best_acc = 0\n",
    "model.train()\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(params['epochs']):\n",
    "    model.train()\n",
    "    sum_loss = 0.0\n",
    "    total = 0\n",
    "\n",
    "    # batch loop\n",
    "    for i, x in enumerate(train_loader):\n",
    "        inputs = x['data'].to(device)\n",
    "        lengths = x['lengths']\n",
    "        labels = x['label'].to(device)\n",
    "        # zero accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output = model(inputs, lengths)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = loss_fn(output, labels.long())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        clip_grad_norm_(model.parameters(), max_norm=params['gclip'])\n",
    "        optimizer.step()\n",
    "\n",
    "        sum_loss += loss.item()*x['label'].shape[0]\n",
    "        total += x['label'].shape[0]\n",
    "               \n",
    "        # loss stats\n",
    "        if (params['print_every'] is not None) and (i % params['print_every'] == 0):\n",
    "            print('Step {} / {}, Loss {}'.format(i, len(train_loader), loss.item()))\n",
    "    # Learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Get validation loss\n",
    "    val_loss, val_acc, val_rmse, _ = validation_metrics(model, val_loader)\n",
    "    \n",
    "    writer.add_scalar(\"Train Loss\", sum_loss/total, e)\n",
    "    writer.add_scalar(\"Validation Loss\", val_loss, e)\n",
    "    writer.add_scalar(\"Validation Accuracy\", val_acc, e)\n",
    "    writer.add_scalar(\"Validation RMSE\", val_rmse, e)\n",
    "\n",
    "    train_loss_history.append(sum_loss/total)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print(\"Epoch: {}/{}...\".format(e+1, params['epochs']),\n",
    "            \"Train Loss: {:.3f}...\".format(sum_loss/total),\n",
    "            \"Val Loss: {:.3f}...\".format(val_loss),\n",
    "            \"Val Acc: {:.3f}...\".format(val_acc),\n",
    "            \"Val RMSE: {:.3f}\".format(val_rmse))\n",
    "    # Early stopping\n",
    "    if val_acc > best_acc:\n",
    "        pat = 0\n",
    "        best_acc = val_acc\n",
    "        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        best_epoch = e+1\n",
    "    else:\n",
    "        pat += 1\n",
    "\n",
    "    if pat == params['patience']:\n",
    "        print('Early stopping at epoch {epoch}.'.format(epoch=best_epoch))\n",
    "        break\n",
    "            \n",
    "model.load_state_dict(best_model_weights)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/25... Train Loss: 0.926... Val Loss: 0.806... Val Acc: 0.637... Val RMSE: 0.740\n",
      "Epoch: 2/25... Train Loss: 0.853... Val Loss: 0.730... Val Acc: 0.672... Val RMSE: 0.701\n",
      "Epoch: 3/25... Train Loss: 0.802... Val Loss: 0.751... Val Acc: 0.672... Val RMSE: 0.738\n",
      "Epoch: 4/25... Train Loss: 0.725... Val Loss: 0.710... Val Acc: 0.694... Val RMSE: 0.689\n",
      "Epoch: 5/25... Train Loss: 0.704... Val Loss: 0.796... Val Acc: 0.682... Val RMSE: 0.728\n",
      "Epoch: 6/25... Train Loss: 0.672... Val Loss: 0.700... Val Acc: 0.694... Val RMSE: 0.682\n",
      "Epoch: 7/25... Train Loss: 0.663... Val Loss: 0.767... Val Acc: 0.690... Val RMSE: 0.708\n",
      "Epoch: 8/25... Train Loss: 0.639... Val Loss: 0.731... Val Acc: 0.701... Val RMSE: 0.673\n",
      "Epoch: 9/25... Train Loss: 0.619... Val Loss: 0.672... Val Acc: 0.716... Val RMSE: 0.634\n",
      "Epoch: 10/25... Train Loss: 0.614... Val Loss: 0.704... Val Acc: 0.703... Val RMSE: 0.661\n",
      "Epoch: 11/25... Train Loss: 0.604... Val Loss: 0.709... Val Acc: 0.704... Val RMSE: 0.662\n",
      "Epoch: 12/25... Train Loss: 0.585... Val Loss: 0.667... Val Acc: 0.718... Val RMSE: 0.623\n",
      "Epoch: 13/25... Train Loss: 0.574... Val Loss: 0.669... Val Acc: 0.716... Val RMSE: 0.628\n",
      "Epoch: 14/25... Train Loss: 0.557... Val Loss: 0.633... Val Acc: 0.713... Val RMSE: 0.610\n",
      "Epoch: 15/25... Train Loss: 0.548... Val Loss: 0.685... Val Acc: 0.715... Val RMSE: 0.621\n",
      "Epoch: 16/25... Train Loss: 0.533... Val Loss: 0.660... Val Acc: 0.724... Val RMSE: 0.606\n",
      "Epoch: 17/25... Train Loss: 0.520... Val Loss: 0.664... Val Acc: 0.718... Val RMSE: 0.616\n",
      "Epoch: 18/25... Train Loss: 0.504... Val Loss: 0.670... Val Acc: 0.727... Val RMSE: 0.579\n",
      "Epoch: 19/25... Train Loss: 0.500... Val Loss: 0.661... Val Acc: 0.718... Val RMSE: 0.600\n",
      "Epoch: 20/25... Train Loss: 0.482... Val Loss: 0.690... Val Acc: 0.723... Val RMSE: 0.597\n",
      "Epoch: 21/25... Train Loss: 0.458... Val Loss: 0.665... Val Acc: 0.729... Val RMSE: 0.568\n",
      "Epoch: 22/25... Train Loss: 0.459... Val Loss: 0.671... Val Acc: 0.722... Val RMSE: 0.563\n",
      "Epoch: 23/25... Train Loss: 0.441... Val Loss: 0.674... Val Acc: 0.730... Val RMSE: 0.574\n",
      "Epoch: 24/25... Train Loss: 0.423... Val Loss: 0.691... Val Acc: 0.732... Val RMSE: 0.584\n",
      "Epoch: 25/25... Train Loss: 0.416... Val Loss: 0.699... Val Acc: 0.723... Val RMSE: 0.568\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Open Tensorboard GUI to see the logs:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir runs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's plot the loss curves in one figure to have an insight of the training. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "fig = plt.figure()\n",
    "plt.title('Loss curves')\n",
    "plt.plot(train_loss_history, '-', label='train')\n",
    "plt.plot(val_loss_history, '-', label='val')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Iteration')\n",
    "writer.add_figure(f\"Loss curves\", fig)\n",
    "fig"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Model\n",
    "\n",
    "As we trained a model and improved it on the validation set, we can now test it on the test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], collate_fn=collate, drop_last=False)\n",
    "\n",
    "_, test_acc, test_rmse, y_pred = validation_metrics(model, test_loader)\n",
    "y_true = [x['label'].item() for x in test_dataset]\n",
    "report = classification_report(y_true, y_pred, target_names=['negative', 'neutral', 'positive'], output_dict=True)\n",
    "print(classification_report(y_true, y_pred, target_names=['negative', 'neutral', 'positive']))\n",
    "\n",
    "writer.add_hparams(params,{\"macro_acc\":test_acc, \"mse\": test_rmse, \"f1-score\": report['macro avg']['f1-score'],\n",
    "                         \"precision\":report['macro avg']['precision'], \"recall\":report['macro avg']['recall']})\n",
    "writer.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.63      0.87      0.73       643\n",
      "     neutral       0.70      0.53      0.60      1644\n",
      "    positive       0.73      0.79      0.76      1811\n",
      "\n",
      "    accuracy                           0.70      4098\n",
      "   macro avg       0.69      0.73      0.70      4098\n",
      "weighted avg       0.70      0.70      0.69      4098\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "\n",
    "Now that we trained a sufficiently good sentiment classifier, let's run the below cell and type some text to see some predictions (type exit to quit the demo). Since we used a small data, don't expect too much :)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "text = ''\n",
    "w2i = vocab\n",
    "while True:\n",
    "    text = input()\n",
    "    if text == 'exit':\n",
    "        break\n",
    "    \n",
    "    words = torch.tensor([\n",
    "        w2i.get(word, w2i['<unk>'])\n",
    "        for word in nltk.word_tokenize(text)\n",
    "    ]).long().to(device).view(-1, 1)  # T x B\n",
    "    with torch.no_grad():\n",
    "        pred = model(words).cpu()\n",
    "        probs = F.softmax(pred, dim=1)\n",
    "    prob, idx = torch.max(probs, dim=1)\n",
    "    if idx == 0:\n",
    "        txt = ':('\n",
    "    elif idx == 1:\n",
    "        txt = ':/'\n",
    "    elif idx == 2:\n",
    "        txt = ':)'\n",
    "        \n",
    "    print('Input text: {}\\n'.format(text),\n",
    "        'Sentiment -> {}, Confidence -> {:.3f}% '.format(txt, *prob*100))\n",
    "    print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input text: awesome but expensive\n",
      " Sentiment -> :), Confidence -> 71.865% \n",
      "\n",
      "Input text: expensive but awesome\n",
      " Sentiment -> :/, Confidence -> 42.852% \n",
      "\n",
      "Input text: comfortable place polite staff\n",
      " Sentiment -> :/, Confidence -> 75.686% \n",
      "\n",
      "Input text: loved it rooms could have been cleaner but food was great\n",
      " Sentiment -> :), Confidence -> 84.820% \n",
      "\n",
      "Input text: loved it\n",
      " Sentiment -> :), Confidence -> 84.636% \n",
      "\n",
      "Input text: disgusting rooms, unbearable smell\n",
      " Sentiment -> :(, Confidence -> 86.970% \n",
      "\n",
      "Input text: totally worth the money, staff helpful, had a great time\n",
      " Sentiment -> :), Confidence -> 83.638% \n",
      "\n",
      "Input text: kids loved it, great place to rest\n",
      " Sentiment -> :), Confidence -> 81.685% \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}