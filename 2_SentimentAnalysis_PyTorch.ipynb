{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0c0a2f5f2ec388a4b45beb4ef144deae29e8f9d03c06f4c2bd14f758dcb12937e",
   "display_name": "Python 3.7.10 64-bit ('nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Sentiment Analysis 2 - PyTorch\n",
    "\n",
    "## 0. Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "\n",
    "# NLP packages\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Modelling packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from src.text_classifiers import RNNClassifier\n",
    "from src.tripadvisor_dataset import(\n",
    "    SentimentDataset,\n",
    "    collate\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "source": [
    "## 1. Dataset\n",
    "### Load Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              Review  Rating  \\\n",
       "0  nice hotel expensive parking got good deal sta...       4   \n",
       "1  ok nothing special charge diamond member hilto...       2   \n",
       "2  nice rooms not 4* experience hotel monaco seat...       3   \n",
       "3  unique, great stay, wonderful time hotel monac...       5   \n",
       "4  great stay great stay, went seahawk game aweso...       5   \n",
       "\n",
       "                                 Review_preprocessed  Sentiment_rating  \n",
       "0  nice hotel expensive parking got good deal sta...                 1  \n",
       "1  ok nothing special charge diamond member hilto...                 0  \n",
       "2  nice room experience hotel monaco seattle good...                 1  \n",
       "3  unique great stay wonderful time hotel monaco ...                 2  \n",
       "4  great stay great stay went seahawk game awesom...                 2  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Review</th>\n      <th>Rating</th>\n      <th>Review_preprocessed</th>\n      <th>Sentiment_rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>nice hotel expensive parking got good deal sta...</td>\n      <td>4</td>\n      <td>nice hotel expensive parking got good deal sta...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ok nothing special charge diamond member hilto...</td>\n      <td>2</td>\n      <td>ok nothing special charge diamond member hilto...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>nice rooms not 4* experience hotel monaco seat...</td>\n      <td>3</td>\n      <td>nice room experience hotel monaco seattle good...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>unique, great stay, wonderful time hotel monac...</td>\n      <td>5</td>\n      <td>unique great stay wonderful time hotel monaco ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>great stay great stay, went seahawk game aweso...</td>\n      <td>5</td>\n      <td>great stay great stay went seahawk game awesom...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "data_root = os.path.join(os.path.abspath(os.getcwd()), \"data\", \"tripadvisor_hotel_reviews_preproc.csv\")\n",
    "data = pd.read_csv(data_root)\n",
    "data.head()"
   ]
  },
  {
   "source": [
    "### Create Vocabulary\n",
    "\n",
    "Build a vocabulary that only consider the top `max_features` ordered by term frequency across the corpus."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "tfidfconverter = TfidfVectorizer(use_idf=True, max_features=max_features, min_df=0, max_df=1.0)\n",
    "tfidfconverter.fit_transform(data['Review_preprocessed'])\n",
    "pre_vocab = tfidfconverter.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary size: 10000 \n\n  Sample words\n--------------------\n outdoors\n tiresome\n nikko\n contains\n proof\n tablet\n identify\n replenish\n groom\n bummer\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size:', len(pre_vocab), '\\n\\n  Sample words\\n{}'.format('-' * 20))\n",
    "sample_words = random.sample(list(pre_vocab.keys()), 10)\n",
    "for word in sample_words:\n",
    "    print(' {}'.format(word))"
   ]
  },
  {
   "source": [
    "In addition to the words that appear in our data, we need to have two special words:\n",
    "\n",
    "- `<eos>` End of sequence symbol used for padding\n",
    "- `<unk>` Words unknown in our vocabulary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary size: 10002\n"
     ]
    }
   ],
   "source": [
    "vocab = {'<eos>': 0, '<unk>': 1}\n",
    "for key in pre_vocab.keys():\n",
    "    vocab[key] = len(vocab)\n",
    "print('Vocabulary size:', len(vocab))\n",
    "# for i in range(len(vocab)-50, len(vocab)):\n",
    "#     print(list(vocab.keys())[list(vocab.values()).index(i)])"
   ]
  },
  {
   "source": [
    "### Create Dataset\n",
    "\n",
    "Steps:\n",
    "* Tokenize data\n",
    "* Create index-label pairs based on vocabulary\n",
    "* Divide into train, validation and test sets\n",
    "* Wrapping to PyTorch dataset\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(['people', 'talking', 'ca', 'believe', 'excellent', 'rating', 'hotel', 'yes', 'patricia', 'extremely', 'helpful', 'fluent', 'language', 'go', 'way', 'make', 'welcome', 'said', 'place', 'bit', 'dump', 'inexpensive', 'hotel', 'expensive', 'city', 'place', 'bit', 'dated', 'institutional', 'odor', 'charm', 'funeral', 'home', 'walking', 'step', 'hotel', 'girlfriend', 'step', 'condom', 'yes', 'condom', 'step', 'lot', 'guy', 'hanging', 'desk', 'hallway', 'girlfriend', 'swears', 'house', 'prostitution', 'patricia', 'arrange', 'taxi', 'following', 'morning', 'stayed', 'night', 'wrong', 'information', 'cost', 'fare', 'room', 'clean', 'large', 'bathroom', 'small', 'passable', 'night', 'glad', 'leave', 'following', 'morning', 'recommend', 'extended', 'stay', 'unless', 'tight', 'budget', 'care', 'look', 'feel', 'place'], 0)\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = []\n",
    "for i in range(len(data)):\n",
    "    tokenized_data.append((nltk.word_tokenize(data['Review_preprocessed'][i]), data['Sentiment_rating'][i]))\n",
    "print(tokenized_data[-1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "([49, 50, 625, 1353, 321, 1267, 3, 936, 8650, 770, 148, 8227, 385, 1578, 282, 175, 469, 98, 440, 937, 2036, 2679, 3, 4, 388, 440, 937, 2633, 1, 2023, 587, 1, 559, 66, 1154, 3, 380, 1154, 5643, 936, 5643, 1154, 541, 2008, 1305, 96, 51, 380, 1, 428, 1, 8650, 250, 368, 1001, 42, 447, 41, 875, 1669, 783, 1059, 27, 28, 217, 92, 538, 5590, 41, 2499, 409, 1001, 42, 441, 2098, 9, 827, 1958, 1301, 815, 295, 357, 440], 0)\n"
     ]
    }
   ],
   "source": [
    "indexed_data = []\n",
    "for tokens, label in tokenized_data:\n",
    "    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]    \n",
    "    # the token that is not in vocab get assigned <unk>\n",
    "    indexed_data.append((indices, label))\n",
    "print(indexed_data[-1])    "
   ]
  },
  {
   "source": [
    "Dataset class also reverse sorts the sequences with respect to the lengths. Thanks to this sorting, we can reduce the total number of padded elements, which means that we have less computations for padded values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = [(raw_text, tokens, indices, label)\n",
    "    for (raw_text, label), (tokens, _), (indices, _)\n",
    "    in zip(list(data[['Review_preprocessed', 'Sentiment_rating']].to_records(index=False)), tokenized_data, indexed_data)]"
   ]
  },
  {
   "source": [
    "Let's divide the dataset into train, validation and test sets (60%-20%-20%). Stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "val_test_len = int(len(combined_data) * 0.2)\n",
    "train_val_data, test_data = train_test_split(combined_data,test_size=val_test_len, random_state=SEED, stratify=data['Sentiment_rating'])\n",
    "df = pd.DataFrame(train_val_data, columns=['raw_text', 'tokens', 'indices', 'label'])\n",
    "train_data, val_data = train_test_split(train_val_data,test_size=val_test_len, random_state=SEED, stratify=df['label'])"
   ]
  },
  {
   "source": [
    "Let's check out the distribution of the classes in the created datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'0': 0.15684934849446097, '1': 0.4012981308867308, '2': 0.44185252061880825}\n{'0': 0.15683523455133289, '1': 0.4013298359055695, '2': 0.44183492954309767}\n{'0': 0.15681171207808053, '1': 0.40130134200894674, '2': 0.44188694591297273}\n{'0': 0.15690580771107857, '1': 0.40141532454856027, '2': 0.44167886774036114}\n{'0': 0.15690580771107857, '1': 0.4011713030746706, '2': 0.4419228892142509}\n"
     ]
    }
   ],
   "source": [
    "def calc_distr(combined_data):\n",
    "    count={'0':0,'1':0,'2':0}\n",
    "    for raw_text, tokens, indices, label in combined_data:\n",
    "        if label==0:\n",
    "            count['0']+=1\n",
    "        elif label==1:\n",
    "            count['1']+=1\n",
    "        elif label==2:\n",
    "            count['2']+=1\n",
    "    sum = count['0']+count['1']+count['2']\n",
    "    count['0'] = count['0']/sum\n",
    "    count['1'] = count['1']/sum\n",
    "    count['2'] = count['2']/sum\n",
    "    print(count)\n",
    "\n",
    "calc_distr(combined_data)\n",
    "calc_distr(train_val_data)\n",
    "calc_distr(train_data)\n",
    "calc_distr(val_data)\n",
    "calc_distr(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20491\n16393\n12295\n4098\n4098\n"
     ]
    }
   ],
   "source": [
    "# Check out the number of data samples in the created datasets.\n",
    "print(len(combined_data))\n",
    "print(len(train_val_data))\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "source": [
    "Let us wrap our data in a PyTorch dataset. For more details, check out the previous notebook and the corresponding dataset class defined in `src/tripadvisor_dataset.py`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Dataset Class for train, val and test set\n",
    "train_dataset = SentimentDataset(train_data)\n",
    "val_dataset = SentimentDataset(val_data)\n",
    "test_dataset = SentimentDataset(test_data)"
   ]
  },
  {
   "source": [
    "Note that in the dataset we created, not all sequences have the same length. Therefore, we cannot minibatch the data trivially. This means we cannot use a `DataLoader` class easily.\n",
    "\n",
    "To solve the problem, we need to pad the sequences with <code> <eos> </code> tokens that we indexed as zero. To integrate this approach into the Pytorch <a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\" target=\"_blank\">Dataloader</a> class, we will make use of the <code>collate_fn</code> argument. For more details, check out the <code>collate</code> function in <code>src/tripadvisor_dataset</code>.\n",
    "\n",
    "In addition, we use the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\" target=\"_blank\">pad_sequence</a> that pads shorter sequences with 0.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data: \n tensor([[2399,  120,  174],\n        [  17,   84,  156],\n        [ 930,    7, 5148],\n        ...,\n        [  77,    0,    0],\n        [ 510,    0,    0],\n        [ 559,    0,    0]])\n\nLabels: \n tensor([1., 2., 1.])\n\nSequence Lengths: \n tensor([1863, 1673, 1381])\n\n\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(train_dataset, batch_size=3, collate_fn=collate)\n",
    "for batch in loader:\n",
    "    print('Data: \\n', batch['data'])\n",
    "    print('\\nLabels: \\n', batch['label'])\n",
    "    print('\\nSequence Lengths: \\n', batch['lengths'])\n",
    "    print('\\n')\n",
    "    break"
   ]
  },
  {
   "source": [
    "## 2. Creating a Sentiment Classifier\n",
    "\n",
    "After we have loaded the data, it is time to define a model and start training and testing.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "Since we need to predict positive, neutral or negative, we use `cross-entropy loss` to train our model. \n",
    "\n",
    "We usually take *accuracy* as our metric for most classification problems, however, ratings are ordered, *RMSE* (root mean squared error) is a reasonable alternative. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_metrics (model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for i, x in enumerate(data_loader):\n",
    "        input = x['data'].to(device)\n",
    "        lengths = x['lengths']\n",
    "        label = x['label'].to(device)\n",
    "        pred = model(input, lengths)\n",
    "        loss = loss_fn(pred, label.long())\n",
    "        pred = torch.max(pred, 1)[1]\n",
    "        correct += (pred == label).float().sum()\n",
    "        total += label.shape[0]\n",
    "        sum_loss += loss.item()*label.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred.cpu(), label.unsqueeze(-1).cpu()))*label.shape[0]\n",
    "    return sum_loss/total, correct/total, sum_rmse/total"
   ]
  },
  {
   "source": [
    "### Design the model\n",
    "\n",
    "See <code>src/text_classifiers.py</code> for the implementation of the <code>RNNClassifier</code>."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNClassifier(num_embeddings=len(vocab), embedding_dim=128, hidden_size=256, use_lstm=True, n_layers = 5, dropout = 0.4, output_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RNNClassifier(\n  (embedding): Embedding(10002, 128, padding_idx=0)\n  (rnn): LSTM(128, 256, num_layers=5, dropout=0.4)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=3, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "source": [
    "### Train the model\n",
    "\n",
    "Note the **collate function** used with the `DataLoader`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda...\n\n"
     ]
    }
   ],
   "source": [
    "# Training configs\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "print('Using {}...\\n'.format(device))\n",
    "\n",
    "# Move model to the device we are using\n",
    "model = model.to(device)\n",
    "\n",
    "# To tackle the exploding gradient problem, set gclip and use clip_grad_norm_(X, gclip)\n",
    "gclip = 40\n",
    "\n",
    "# Dataloaders, note the collate function\n",
    "batch_size=8\n",
    "train_loader = DataLoader(\n",
    "  train_dataset, batch_size=batch_size, collate_fn=collate, drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "  val_dataset, batch_size=batch_size, collate_fn=collate, drop_last=False\n",
    ")"
   ]
  },
  {
   "source": [
    "Training loop:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 0 / 1536, Loss 1.069003939628601\n",
      "Step 500 / 1536, Loss 1.0757766962051392\n",
      "Step 1000 / 1536, Loss 1.2572004795074463\n",
      "Step 1500 / 1536, Loss 0.5723262429237366\n",
      "Epoch: 1/10... Train Loss: 0.929... Val Loss: 0.826... Val Acc: 0.587... Val RMSE: 0.713\n",
      "Step 0 / 1536, Loss 1.1430671215057373\n",
      "Step 500 / 1536, Loss 0.6208348870277405\n",
      "Step 1000 / 1536, Loss 0.819493293762207\n",
      "Step 1500 / 1536, Loss 0.37319520115852356\n",
      "Epoch: 2/10... Train Loss: 0.735... Val Loss: 0.710... Val Acc: 0.664... Val RMSE: 0.583\n",
      "Step 0 / 1536, Loss 0.9932306408882141\n",
      "Step 500 / 1536, Loss 0.5460716485977173\n",
      "Step 1000 / 1536, Loss 0.5637611150741577\n",
      "Step 1500 / 1536, Loss 0.5697861909866333\n",
      "Epoch: 3/10... Train Loss: 0.600... Val Loss: 0.661... Val Acc: 0.698... Val RMSE: 0.550\n",
      "Step 0 / 1536, Loss 0.9087313413619995\n",
      "Step 500 / 1536, Loss 0.40725064277648926\n",
      "Step 1000 / 1536, Loss 0.45220327377319336\n",
      "Step 1500 / 1536, Loss 0.32335105538368225\n",
      "Epoch: 4/10... Train Loss: 0.497... Val Loss: 0.705... Val Acc: 0.680... Val RMSE: 0.569\n",
      "Step 0 / 1536, Loss 1.1171504259109497\n",
      "Step 500 / 1536, Loss 0.20604702830314636\n",
      "Step 1000 / 1536, Loss 0.14045391976833344\n",
      "Step 1500 / 1536, Loss 0.6658959984779358\n",
      "Epoch: 5/10... Train Loss: 0.392... Val Loss: 0.806... Val Acc: 0.678... Val RMSE: 0.562\n",
      "Early stopping in epoch 4.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "lr=0.0009\n",
    "epochs = 10\n",
    "print_every = 500\n",
    "patience = 2\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_acc = 0\n",
    "model.train()\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    sum_loss = 0.0\n",
    "    total = 0\n",
    "\n",
    "    # batch loop\n",
    "    for i, x in enumerate(train_loader):\n",
    "        inputs = x['data'].to(device)\n",
    "        lengths = x['lengths']\n",
    "        labels = x['label'].to(device)\n",
    "        # zero accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output = model(inputs, lengths)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = loss_fn(output, labels.long())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        clip_grad_norm_(model.parameters(), max_norm=gclip)\n",
    "        optimizer.step()\n",
    "\n",
    "        sum_loss += loss.item()*x['label'].shape[0]\n",
    "        total += x['label'].shape[0]\n",
    "               \n",
    "        # loss stats\n",
    "        if i % print_every == 0:\n",
    "            print('Step {} / {}, Loss {}'.format(i, len(train_loader), loss.item()))\n",
    "    # Get validation loss\n",
    "    val_loss, val_acc, val_rmse = validation_metrics(model, val_loader)\n",
    "\n",
    "    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "            \"Train Loss: {:.3f}...\".format(sum_loss/total),\n",
    "            \"Val Loss: {:.3f}...\".format(val_loss),\n",
    "            \"Val Acc: {:.3f}...\".format(val_acc),\n",
    "            \"Val RMSE: {:.3f}\".format(val_rmse))\n",
    "    # Early stopping\n",
    "    if val_acc > best_acc:\n",
    "        pat = 0\n",
    "        best_acc = val_acc\n",
    "        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    else:\n",
    "        pat += 1\n",
    "\n",
    "    if pat == patience:\n",
    "        print('Early stopping in epoch {epoch}.'.format(epoch=e))\n",
    "        break\n",
    "            \n",
    "model.load_state_dict(best_model_weights)"
   ]
  },
  {
   "source": [
    "## Testing the Model\n",
    "\n",
    "As we trained a model and improved it on the validation set, we can now test it on the test set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy on test set: 0.6952171921730042\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate, drop_last=False)\n",
    "\n",
    "print(\"accuracy on test set: {}\".format(validation_metrics(model, test_loader)[1]))"
   ]
  },
  {
   "source": [
    "## Demo\n",
    "\n",
    "Now that we trained a sufficiently good sentiment classifier, let's run the below cell and type some text to see some predictions (type exit to quit the demo). Since we used a small data, don't expect too much :)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input text: expensive but awesome\n",
      " Sentiment -> :/, Confidence -> 64.667% \n",
      "\n",
      "Input text: awesome but expensive\n",
      " Sentiment -> :), Confidence -> 61.647% \n",
      "\n",
      "Input text: expensive and awesome\n",
      " Sentiment -> :/, Confidence -> 64.667% \n",
      "\n",
      "Input text: awesome but too expensive\n",
      " Sentiment -> :), Confidence -> 71.515% \n",
      "\n",
      "Input text: Awesome but too expensive\n",
      " Sentiment -> :(, Confidence -> 41.919% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "w2i = vocab\n",
    "while True:\n",
    "    text = input()\n",
    "    if text == 'exit':\n",
    "        break\n",
    "    \n",
    "    words = torch.tensor([\n",
    "        w2i.get(word, w2i['<unk>'])\n",
    "        for word in nltk.word_tokenize(text)\n",
    "    ]).long().to(device).view(-1, 1)  # T x B\n",
    "    with torch.no_grad():\n",
    "        pred = model(words).cpu()\n",
    "        probs = F.softmax(pred, dim=1)\n",
    "    prob, idx = torch.max(probs, dim=1)\n",
    "    if idx == 0:\n",
    "        txt = ':('\n",
    "    elif idx == 1:\n",
    "        txt = ':/'\n",
    "    elif idx == 2:\n",
    "        txt = ':)'\n",
    "        \n",
    "    print('Input text: {}\\n'.format(text),\n",
    "        'Sentiment -> {}, Confidence -> {:.3f}% '.format(txt, *prob*100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}